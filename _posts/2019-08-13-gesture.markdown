---
title: "\U0001F44B Exploring hand gestural control for mobile devices"
layout: post
date: 2019-08-11 22:00
tag:
- Cross-device
- Data Visualisation
- Statistical test
- Exit Interview
- Questionnaires
- Mixed-method
projects: true
category: project
author: chloeng
description: Gesture Elicitation
---

## Eliciting User-defined Touch and Mid-air Gestures for Co-located Mobile Gaming
<b>
<b>Nature: </b> Academic research

<b>
<b>Type: </b> Individual project

<b>
<b>Methodologies: </b> Questionnaires, interviews, think-aloud, survey, thematic analysis, data visualisation, descriptive statistics, t-tests

<b>
<b>My role: </b> I managed the entire project starting from literature review of past gesture elicitation studies and gestural control. Following the study design, which includes survey design, artifact design, interview script, I went on recruiting participants and carrying out the lab sessions. After collecting video recordings of the elicitation and survey response, I coded the quant/qual data for analysis, subsequently writing up the report.

<b>
<b>Duration: </b> 6 months
<br>
<br>
<br>
![example gestures](https://chloenhy.github.io/assets/images/gesture/example-gesture.jpg)
<figcaption>Examples of mid-air gestures from the study</figcaption>
<br>
<br>
### Background
<b>
In recent years, mobile games have become increasingly popular and have largely improved on their interaction techniques. This improvement is enabled by the increasing capability in modern mobile devices as they feature sophisticated sensors such as accelerometers, gyroscope, and motion sensors, which allows for a vast range of input methods. 

<b>
To make use of the improving capability in mobile devices in the domain of mobile games, there has been research that explores alternative input methods (other than using capacitive touchscreens).

<b>
In this research, I aim <span class="evidence">to explore the use of gesture controls in co-located mobile gaming, an area that has not been focused on in the industry and research community</span>. I will explore traditional multiplayer tabletop games such as board and card games due to their clearly defined game tasks, and the communicative nature of the game, and the materiality of the game materials cherished by players in a co-located setting.
<br>
<br>
<br>
<br>
### Research Approach
<b>
I draw upon the widely adopted gesture elicitation methodology to understand user mental models and help develop user-defined gestures. 

<b>User elicitation is one form of participatory design to include usersâ€™ mental models and proposals in designing new interaction techniques. <span class="evidence">Elicitation studies aim to invoke easy-to-learn and memorable user-defined gestures</span> instead of gestures that are optimised for machine recognition. In an elicitation, participants are asked to propose gestures to achieve tasks (known as referents) in a specified modality.

<b>
I conducted a gesture elicitation study for tasks common in a multiplayer card and board game moderated by mobile devices. I recruited 24 participants in pairs. Twelve were working professionals different backgrounds such as analytics, marketing, engineering, clinical settings, and sports. The other twelve were university students in various disciplines.


<div style="display: flex; justify-content: center;">
<img alt="participants" src="https://chloenhy.github.io/assets/images/gesture/participants.jpg" width="60%" height="60%" align="middle"/>
</div>
<br>
<figcaption>One of the participant pairs proposing gestures</figcaption>
<br>
<br>
<b>Research question: 

<b><b>How can the results and observations made in a gesture elicitation for game tasks inform gesture design for co-located multiplayer mobile games?</b>
<br>
<br>
<br>
<br>
### Data Collection and Analysis
<b>Data collected include <b>observational notes</b> jotted by the experimenter and the <b>video recordings</b> of the entire sessions, including the elicitation and the post-study interviews. The <b>pre-</b> and <b>post-study questionnaires</b> contain demographic, technology usage habits, Likert-scale and free-form responses. <b>Descriptive statistics</b> were generated from the Likert-scale questions. 

<b>All gestures were coded and fed into the <a href="http://depts.washington.edu/acelab/proj/dollar/agate.html" target="_blank">AGAte 2.0 tool</a> for <b>agreement rate</b> calculation.
<br>
<br>
![AGAte 2.0 tool screenshot](https://chloenhy.github.io/assets/images/gesture/agate.jpg)
<figcaption>Screenshot of the AGAte software for analysing the agreement rates among gestures</figcaption>
<br>
<br>
<br>
<br>
### Results
#### Quantitative 
<b>A total of 662 gesture proposals, with 286 distinct gestures (by referents) were collected. t-tests were performed to compare gesture proposals between Mid-air and Touch modality.

![bar chart of the agreement rates](https://chloenhy.github.io/assets/images/gesture/agreement-rates.jpg)


<div style="display: flex; justify-content: center;">
  <img alt="survey" src="https://chloenhy.github.io/assets/images/gesture/gesture-survey.jpg" width="60%" height="60%" align="middle"/>
</div>

#### Qualitative 
<div style="display: flex; justify-content: center;">
    <img alt="example gestures" src="https://chloenhy.github.io/assets/images/gesture/collab-gesture.jpg" width="50%" height="50%" />
</div>
<div style="display: flex; justify-content: center;">
<img alt="mid air gestures" src="https://chloenhy.github.io/assets/images/gesture/mid-air-gesture.jpg"/>
</div>
<div style="display: flex; justify-content: center;">
<img alt="touch gestures" src="https://chloenhy.github.io/assets/images/gesture/touch-gesture.jpg"/>
</div>




The detailed explanation of the agreement rates, taxonomy and gestures are presented in the full paper. Feel free to drop me a message if you are interested in the full paper :)